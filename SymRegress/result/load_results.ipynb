{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from scipy import stats\n",
    "from sklearn.utils.random import check_random_state\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pickle\n",
    "\n",
    "def summarize_rule(rule_ls,show):\n",
    "    string_rule = rule_ls\n",
    "    group = []\n",
    "    count = []\n",
    "    loss_info = []\n",
    "    \n",
    "    for i in range(len(string_rule)):\n",
    "        if_inside = string_rule[i] in group\n",
    "        if not if_inside:\n",
    "            group.append(string_rule[i])\n",
    "            count.append(1)\n",
    "        else:\n",
    "            idx = group.index(string_rule[i])\n",
    "            count[idx] += 1\n",
    "    if show==True:\n",
    "        for i in range(len(group)):\n",
    "            print(group[i],\"\\t\",round(count[i]/len(string_rule)*100),\"%\")\n",
    "    else:\n",
    "        return group, np.array(count)/len(string_rule)\n",
    "        \n",
    "def summarize_loss(rule_ls,loss_best_ls):\n",
    "    string_rule = rule_ls\n",
    "    group = []\n",
    "    loss_info = []\n",
    "\n",
    "    for i in range(len(string_rule)):\n",
    "        if_inside = string_rule[i] in group\n",
    "        if not if_inside:\n",
    "            group.append(string_rule[i])\n",
    "            loss_info.append(loss_best_ls[i])\n",
    "        else:\n",
    "            idx = group.index(string_rule[i])\n",
    "            loss_info[idx] = np.min([loss_info[idx],loss_best_ls[i]])\n",
    "    for i in range(len(group)):\n",
    "        print(group[i],\"\\t\",loss_info[i])\n",
    "            \n",
    "def get_result_sb(string,show=True):\n",
    "    file = string\n",
    "    with open(file+'.pkl', 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "    rule_nonstab = result[0]\n",
    "    rule_stab = result[1]\n",
    "    n_ls = result[2]\n",
    "    group_ls = result[3]\n",
    "    rule_fit0_ls = result[4]\n",
    "    rule_fit_ls = result[5]\n",
    "    loss_best_ls = result[6]\n",
    "    \n",
    "    if show == True:\n",
    "#         print(len(rule_nonstab))\n",
    "        print(\"non stablize:\")\n",
    "        summarize_rule(rule_nonstab,show)\n",
    "        print(\"\\n\")\n",
    "        print(\"stablize:\")\n",
    "        summarize_rule(rule_stab,show)\n",
    "#         print(n_ls)\n",
    "\n",
    "#         print(\"\\n\")\n",
    "#         print(\"test loss info:\")\n",
    "#         summarize_loss(rule_stab,loss_best_ls)\n",
    "        \n",
    "    else:\n",
    "#         print(string)\n",
    "#         print(len(rule_nonstab))\n",
    "        group1, percent1 = summarize_rule(rule_nonstab,show)\n",
    "        group2, percent2 = summarize_rule(rule_stab,show)\n",
    "        \n",
    "        return percent1, percent2\n",
    "    \n",
    "def extract_data(string_ls,feature,file):\n",
    "    percent1_ls = []\n",
    "    percent2_ls = []\n",
    "    for string in string_ls:\n",
    "        percent1, percent2 = get_result_sb(string,False)\n",
    "        percent1_ls.append(np.sort(percent1)[::-1])\n",
    "        percent2_ls.append(np.sort(percent2)[::-1])\n",
    "\n",
    "    df = pd.DataFrame(data={'feature': [], 'rule': [], 'percent':[],'type':[]})\n",
    "\n",
    "    for k in range(len(feature)):\n",
    "        percent1 = percent1_ls[k]\n",
    "        for i in range(len(percent1)):\n",
    "            df.loc[len(df.index)] = [feature[k],i,percent1[i],\"Non-Stab\"]\n",
    "\n",
    "        percent2 = percent2_ls[k]\n",
    "        for i in range(len(percent2)):\n",
    "            df.loc[len(df.index)] = [feature[k],i,percent2[i],\"Stab\"]\n",
    "\n",
    "    df.to_csv(file,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non stablize:\n",
      "X11 + X3 + X4 + 1 \t 66 %\n",
      "X1 + X11 + X3 + X4 + 1 \t 7 %\n",
      "X1 + X3 + X4 \t 16 %\n",
      "X3 + X4 + X8 + 1 \t 5 %\n",
      "X1 + X11 + X3 + X4 + X8 \t 3 %\n",
      "X1 + X3 + X4 + X8 \t 1 %\n",
      "X1 + X3 + X4 + X8 + 1 \t 1 %\n",
      "X1 + X11 + X3 + X4 \t 1 %\n",
      "\n",
      "\n",
      "stablize:\n",
      "X11 + X3 + X4 + 1 \t 83 %\n",
      "X1 + X3 + X4 \t 10 %\n",
      "X1 + X11 + X3 + X4 + X8 \t 4 %\n",
      "X1 + X11 + X3 + X4 + 1 \t 1 %\n",
      "X1 + X11 + X3 + X4 \t 1 %\n",
      "X1 + X3 + X4 + X8 \t 1 %\n"
     ]
    }
   ],
   "source": [
    "get_result_sb(\"symbolic_data1_n_sim_10000_nmax_100000_len_max_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_sim\n",
    "string_ls = [\"symbolic_data1_n_sim_1000_nmax_100000_len_max_3\",\n",
    "            \"symbolic_data1_n_sim_2500_nmax_100000_len_max_3\",\n",
    "            \"symbolic_data1_n_sim_5000_nmax_100000_len_max_3\",\n",
    "            \"symbolic_data1_n_sim_7500_nmax_100000_len_max_3\",\n",
    "            \"symbolic_data1_n_sim_10000_nmax_100000_len_max_3\"]\n",
    "feature = [1000,2500,5000,7500,10000]\n",
    "file = \"symbolic_data1_n_sim.csv\"\n",
    "extract_data(string_ls,feature,file)\n",
    "\n",
    "# len_max\n",
    "string_ls = [\"symbolic_data1_n_sim_10000_nmax_100000_len_max_3\",\n",
    "            \"symbolic_data1_n_sim_10000_nmax_100000_len_max_4\",\n",
    "            \"symbolic_data1_n_sim_10000_nmax_100000_len_max_5\",\n",
    "            \"symbolic_data1_n_sim_10000_nmax_100000_len_max_6\",\n",
    "            \"symbolic_data1_n_sim_10000_nmax_100000_len_max_7\"]\n",
    "feature = [3,4,5,6,7]\n",
    "file = \"symbolic_data1_len_max.csv\"\n",
    "extract_data(string_ls,feature,file)\n",
    "\n",
    "# nmax\n",
    "string_ls = [\"symbolic_data1_n_sim_10000_nmax_10000_len_max_3\",\n",
    "            \"symbolic_data1_n_sim_10000_nmax_25000_len_max_3\",\n",
    "            \"symbolic_data1_n_sim_10000_nmax_50000_len_max_3\",\n",
    "            \"symbolic_data1_n_sim_10000_nmax_100000_len_max_3\",\n",
    "            \"symbolic_data1_n_sim_10000_nmax_250000_len_max_3\"]\n",
    "feature = [10000,25000,50000,100000,250000]\n",
    "file = \"symbolic_data1_nmax.csv\"\n",
    "extract_data(string_ls,feature,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non stablize:\n",
      "X20 + X22 + X27 + X7 \t 6 %\n",
      "X13 + X20 + X22 + X27 \t 1 %\n",
      "X20 + X21 + X22 + X27 \t 16 %\n",
      "X20 + X22 + X23 + X27 \t 48 %\n",
      "X20 + X22 + X26 + X27 \t 1 %\n",
      "X20 + X23 + X27 + X7 \t 2 %\n",
      "X20 + X21 + X22 + X23 \t 2 %\n",
      "X13 + X20 + X23 + X27 \t 2 %\n",
      "X20 + X22 + X27 + X4 \t 1 %\n",
      "X20 + X22 + X23 + X24 \t 1 %\n",
      "X20 + X23 + X27 + 1 \t 1 %\n",
      "X20 + X22 + X23 + X7 \t 4 %\n",
      "X21 + X22 + X23 + X27 \t 3 %\n",
      "X1 + X20 + X22 + X26 \t 1 %\n",
      "X20 + X22 + X23 + 1 \t 1 %\n",
      "X12 + X20 + X22 + X27 \t 1 %\n",
      "X20 + X22 + X27 + X3 \t 1 %\n",
      "X20 + X22 + X27 \t 4 %\n",
      "X1 + X20 + X22 + X27 \t 3 %\n",
      "X20 + X22 + X26 + X7 \t 1 %\n",
      "\n",
      "\n",
      "stablize:\n",
      "X20 + X22 + X23 + X27 \t 95 %\n",
      "X20 + X22 + X27 + X7 \t 1 %\n",
      "X1 + X20 + X22 + X27 \t 1 %\n",
      "X20 + X21 + X22 + X27 \t 2 %\n",
      "X20 + X22 + X27 \t 1 %\n"
     ]
    }
   ],
   "source": [
    "get_result_sb(\"symbolic_data2_n_sim_10000_nmax_100000_len_max_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_sim\n",
    "string_ls = [\"symbolic_data2_n_sim_1000_nmax_100000_len_max_3\",\n",
    "            \"symbolic_data2_n_sim_2500_nmax_100000_len_max_3\",\n",
    "            \"symbolic_data2_n_sim_5000_nmax_100000_len_max_3\",\n",
    "            \"symbolic_data2_n_sim_7500_nmax_100000_len_max_3\",\n",
    "            \"symbolic_data2_n_sim_10000_nmax_100000_len_max_3\"]\n",
    "feature = [1000,2500,5000,7500,10000]\n",
    "file = \"symbolic_data2_n_sim.csv\"\n",
    "extract_data(string_ls,feature,file)\n",
    "\n",
    "# len_max\n",
    "string_ls = [\"symbolic_data2_n_sim_10000_nmax_100000_len_max_3\",\n",
    "            \"symbolic_data2_n_sim_10000_nmax_100000_len_max_4\",\n",
    "            \"symbolic_data2_n_sim_10000_nmax_100000_len_max_5\",\n",
    "            \"symbolic_data2_n_sim_10000_nmax_100000_len_max_6\",\n",
    "            \"symbolic_data2_n_sim_10000_nmax_100000_len_max_7\"]\n",
    "feature = [3,4,5,6,7]\n",
    "file = \"symbolic_data2_len_max.csv\"\n",
    "extract_data(string_ls,feature,file)\n",
    "\n",
    "# nmax\n",
    "string_ls = [\"symbolic_data2_n_sim_10000_nmax_10000_len_max_3\",\n",
    "            \"symbolic_data2_n_sim_10000_nmax_25000_len_max_3\",\n",
    "            \"symbolic_data2_n_sim_10000_nmax_50000_len_max_3\",\n",
    "            \"symbolic_data2_n_sim_10000_nmax_100000_len_max_3\",\n",
    "            \"symbolic_data2_n_sim_10000_nmax_250000_len_max_3\"]\n",
    "feature = [10000,25000,50000,100000,250000]\n",
    "file = \"symbolic_data2_nmax.csv\"\n",
    "extract_data(string_ls,feature,file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable_distill",
   "language": "python",
   "name": "stable_distill"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
